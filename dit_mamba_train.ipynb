{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cf35b01-7a2d-420d-af6c-cda2288e725b",
   "metadata": {},
   "source": [
    "# Mamba Denoiser VS DiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722d232-0dbe-4ecd-a64b-3aa546d30494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implementation is based on Dino Diffusion\n",
    "# https://github.com/madebyollin/dino-diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f0e0ca-8e5c-4f28-bb00-3d924df23f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "from functools import lru_cache\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dit_mamba_trainloader import Config\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0719d11b-c90f-4192-8759-0def67f9cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "Config.image_size = 64\n",
    "Config.shape = (3, 64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e166ad1-17b6-491e-97e8-9f83a0341d87",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16c2d4-e85e-4a9e-ae6c-dd62dd4f532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: make sure to download the dataset in install.ipynb\n",
    "# https://github.com/cyizhuo/Stanford-Cars-dataset\n",
    "\n",
    "Sample = namedtuple(\"Sample\", (\"im\", \"noisy_im\", \"noise_level\"))\n",
    "\n",
    "def alpha_blend(a, b, alpha):\n",
    "    return alpha * a + (1 - alpha) * b\n",
    "\n",
    "class CustomStanfordCarsDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.car_names = sorted(os.listdir(img_dir))  # Assuming car names are the class labels\n",
    "        \n",
    "        for idx, car_name in enumerate(self.car_names):\n",
    "            car_dir = os.path.join(img_dir, car_name)\n",
    "            for img_name in os.listdir(car_dir):\n",
    "                self.image_paths.append(os.path.join(car_dir, img_name))\n",
    "                self.labels.append(idx)  # Use the index of the car name as the label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        noise = torch.rand_like(image)\n",
    "        noise_level = torch.rand(1, 1, 1)\n",
    "        noisy_im = alpha_blend(noise, image, noise_level)\n",
    "        return Sample(image, noisy_im, noise_level)\n",
    "\n",
    "def load_transformed_dataset():\n",
    "    data_transforms = [\n",
    "        transforms.Resize((Config.image_size, Config.image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(), # Scales data into [0,1] \n",
    "        #transforms.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1] \n",
    "    ]\n",
    "    data_transform = transforms.Compose(data_transforms)\n",
    "\n",
    "    train = CustomStanfordCarsDataset('Stanford-Cars-dataset/train', transform=data_transform)\n",
    "\n",
    "    test = CustomStanfordCarsDataset('Stanford-Cars-dataset/test', transform=data_transform)\n",
    "    \n",
    "    return torch.utils.data.ConcatDataset([train, test])\n",
    "def show_tensor_image(image):\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        #transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
    "        transforms.Lambda(lambda t: t * 255.),\n",
    "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "\n",
    "    # Take first image of batch\n",
    "    if not isinstance(image, torch.Tensor) or image.ndim == 4:\n",
    "        image = torch.cat(tuple(image), -1)\n",
    "    display(reverse_transforms(image))\n",
    "\n",
    "d_train = load_transformed_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b881b21f-2160-4e51-9c64-633e10410086",
   "metadata": {},
   "source": [
    "# View the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108fd593-ec13-474d-98e7-a8dd0af14d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_dataset(dataset, n=16):\n",
    "    print(f\"Dataset has {len(dataset)} samples (not counting augmentation).\")\n",
    "    print(f\"Here are some samples from the dataset:\")\n",
    "    samples = random.choices(dataset, k=n)\n",
    "    print(f\"Inputs\")\n",
    "    \n",
    "    show_tensor_image(s.noisy_im for s in samples)\n",
    "    show_tensor_image(s.noise_level.expand(3, 16, Config.image_size) for s in samples)\n",
    "    print(f\"Target Outputs\")\n",
    "    show_tensor_image(s.im for s in samples)\n",
    "demo_dataset(d_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d78e9a-4433-4e1a-a64c-655d7909107f",
   "metadata": {},
   "source": [
    "# Initializing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94525784-0c43-465f-a03c-e9c4b2026063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure n_layers is divisible by 8 for mamba and embed_dim\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "@dataclass\n",
    "class MambaConfig:\n",
    "    image_size: int = 64\n",
    "    patch_size: int = 2\n",
    "    embed_dim: int = 160\n",
    "    dropout: float = 0\n",
    "    n_layers: int = 8\n",
    "    n_channels: int = 3\n",
    "\n",
    "mamba_config = MambaConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92608f71-f0c2-441a-b1fe-859e4c8dbe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_denoiser import MambaDenoiser\n",
    "from dit import DiT\n",
    "\n",
    "mamba_model = MambaDenoiser(**asdict(mamba_config)).to(Config.device)\n",
    "transformer_model = DiT(depth=8, hidden_size=12*32, patch_size=2, num_heads=12, input_size = 64).to(Config.device)\n",
    "\n",
    "\n",
    "print(f\"Mamba model has {sum(p.numel() for p in mamba_model.parameters() if p.requires_grad) / 1e6:.1f} million trainable parameters.\")\n",
    "print(f\"Transformer model has {sum(p.numel() for p in transformer_model.parameters() if p.requires_grad) / 1e6:.1f} million trainable parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b945ed3-815b-4416-92ee-fbe3f6b3595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_average(w_prev, w_new, n): # taking the average prediction of the model\n",
    "    alpha = min(0.9, n / 10)\n",
    "    return alpha_blend(w_prev, w_new, alpha)\n",
    "    \n",
    "avg_mamba_model = torch.optim.swa_utils.AveragedModel(mamba_model, avg_fn=weight_average)\n",
    "avg_transformer_model = torch.optim.swa_utils.AveragedModel(transformer_model, avg_fn=weight_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae7cd9a-f234-43ed-8df0-09fc9e63b883",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f9cd88-34d3-4cfa-814b-a53283dbaef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dit_mamba_trainloader import Trainer, generate_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a8dd1-ecfe-4476-8842-dbfd1bb46652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train mamba\n",
    "mamba_trainer = Trainer(mamba_model, avg_mamba_model, d_train, batch_size=32, learning_rate=3e-4)\n",
    "mamba_trainer.train(n_seconds=6*60*60) # change the training time if necessary \n",
    "torch.save(avg_mamba_model.state_dict(), 'avg_mamba_model.pth')\n",
    "torch.save(mamba_model.state_dict(), 'mamba_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a40bb2-2e10-4189-bb5f-572a17e3f4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Transformer\n",
    "transformer_trainer = Trainer(transformer_model, avg_transformer_model, d_train, batch_size=32)\n",
    "transformer_trainer.train(n_seconds=6*60*60)\n",
    "torch.save(avg_transformer_model.state_dict(), 'avg_transformer_model.pth')\n",
    "torch.save(transformer_model.state_dict(), 'transformer_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf4b61-70b0-4fae-bce0-566ac674381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mamba_model.load_state_dict(torch.load('avg_mamba_model.pth'))\n",
    "mamba_model.load_state_dict(torch.load('mamba_model.pth'))\n",
    "avg_transformer_model.load_state_dict(torch.load('avg_transformer_model.pth'))\n",
    "transformer_model.load_state_dict(torch.load('transformer_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe48eeb4-c6a8-4076-80a5-07ca9108cfab",
   "metadata": {},
   "source": [
    "# Generate the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d9c50-5a76-4dd2-a790-de8bc10e830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_sample_grids(dataset, model, rows=6, cols=6, n_steps=100, step_size=2):\n",
    "    torch.manual_seed(16) # change the seed if necessary \n",
    "    real_rows, fake_rows = [], []\n",
    "    for i in tqdm(range(rows)):\n",
    "        real_rows.append(torch.cat([random.choice(dataset).im for _ in range(cols)], -1))\n",
    "        fake_rows.append(torch.cat(tuple(generate_images(model, n_images=cols, n_steps = n_steps, step_size = step_size)), -1))\n",
    "    real_im = torch.cat(real_rows, -2)\n",
    "    padding = torch.ones_like(real_im[..., :32])\n",
    "    fake_im = torch.cat(fake_rows, -2).cpu()\n",
    "    return TF.to_pil_image(torch.cat([real_im, padding, fake_im], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94204f69-5f40-4e05-9d38-e4794b97b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images for the mamba model:\n",
    "demo_sample_grids(d_train, avg_mamba_model, n_steps=100, step_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b90d0-4318-4b6e-a560-a56d9cc36b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images for the Transformer model:\n",
    "demo_sample_grids(d_train, avg_transformer_model, n_steps=100, step_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a17d39-2a35-4460-9d48-47eaa1bcf9ac",
   "metadata": {},
   "source": [
    "# FID score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ea099-0368-46c1-abf2-d89972d7f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fid import fid_score\n",
    "\n",
    "def calculate_FID(dataset, model, n_steps=300, step_size=1, n_samples=100):\n",
    "    real_list, fake_list = [dataset[i].im for i in range(n_samples)], []\n",
    "    for i in tqdm(range(int(len(real_list)/100))):\n",
    "        f = generate_images(model, n_images=100, n_steps=n_steps, step_size=step_size)\n",
    "        for z in f:\n",
    "            fake_list.append(z)\n",
    "\n",
    "    fid = fid_score(real_list, fake_list, device=Config.device)\n",
    "    return fid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859ce82-c05e-40ef-9818-92b055c4caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mamba_fid = 0\n",
    "with torch.no_grad():\n",
    "    mamba_fid = calculate_FID(d_train, avg_mamba_model, n_steps=100, step_size=3, n_samples=16000)\n",
    "print(\"FID score for mamba model: \", mamba_fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a22d086-2c28-4045-b79a-7cd23cb302d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_fid = 0\n",
    "with torch.no_grad():\n",
    "    transformer_fid = calculate_FID(d_train, avg_transformer_model, n_steps=100, step_size=3, n_samples=16000)\n",
    "print(\"FID score for transformer model: \", transformer_fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1ea22e-b41d-4a4f-b5a1-aea932457df8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
